{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Parikshit Sunil Deshmukh - pdeshmuk - 50247649\n",
    "### Mahalakshmi Padma Sri Harsha Maddu - mmaddu - 50246769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Referances:\n",
    "# https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "##Importing the data\n",
    "\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/media/parik/New Volume/DIC/lab3/Final_Data/Final_Data/output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "##Tokenizer\n",
    "regxTokenizer = RegexTokenizer(inputCol=\"Article\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "##StopWords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"advertisement\")\n",
    "stop_words.add(\"said\")\n",
    "stop_words.add(\"by\")\n",
    "stop_words.add(\"\")\n",
    "stop_words.add(\" \")\n",
    "stop_words.add(\"  \")\n",
    "stop_words.add(\"mr\")\n",
    "stop_words = list(stop_words)\n",
    "newL=[\"advertisement\",\"percent\",\"space\",\"years\",\"another\",\"first\",\"second\",\"third\",\"one\",\"might\",\"two\",\"three\",\"think\",\"right\",\"better\",\"people\",\"women\",\"family\",\"person\",\"trump\",\"woman\",\"article\",\"state\",\"still\",\"including\",\"college\",\"united\",\"states\",\"new\",\"york\",\"later\",\"better\",\"president\",\"office\",\"working\",\"white\",\"house\",\"chief\",\"start\",\"small\",\"world\",\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"]\n",
    "for stp in newL:\n",
    "    stop_words.append(stp)\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stop_words)\n",
    "\n",
    "##Bag of words approach: CountVectors\n",
    "\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "|  3.0|\n",
      "|  2.0|\n",
      "+-----+\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|             Article|Category|               words|            filtered|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|australia office ...|Politics|[australia, offic...|[australia, sex, ...|(1519,[5,8,9,14,1...|  2.0|\n",
      "|italy artists jas...|Politics|[italy, artists, ...|[italy, artists, ...|(1519,[1,5,8,12,1...|  2.0|\n",
      "|art design notebo...|Politics|[art, design, not...|[art, design, not...|(1519,[1,5,8,10,1...|  2.0|\n",
      "|region actress go...|Politics|[region, actress,...|[region, actress,...|(1519,[5,7,10,14,...|  2.0|\n",
      "|byop ed contribut...|Politics|[byop, ed, contri...|[byop, ed, contri...|(1519,[1,2,4,8,9,...|  2.0|\n",
      "|books abortion bo...|Politics|[books, abortion,...|[books, abortion,...|(1519,[1,2,5,8,10...|  2.0|\n",
      "|borderfrom museum...|Politics|[borderfrom, muse...|[borderfrom, muse...|(1519,[1,3,4,5,8,...|  2.0|\n",
      "|byop ed contribut...|Politics|[byop, ed, contri...|[byop, ed, contri...|(1519,[1,3,7,15,1...|  2.0|\n",
      "|book review nonfi...|Politics|[book, review, no...|[book, review, no...|(1519,[1,5,8,10,1...|  2.0|\n",
      "|politics sex scan...|Politics|[politics, sex, s...|[politics, sex, s...|(1519,[2,5,8,9,12...|  2.0|\n",
      "|travel border mus...|Politics|[travel, border, ...|[travel, border, ...|(1519,[1,3,4,5,8,...|  2.0|\n",
      "|bynonfictionshe d...|Politics|[bynonfictionshe,...|[bynonfictionshe,...|(1519,[1,5,8,11,1...|  2.0|\n",
      "|bybooks scarlet a...|Politics|[bybooks, scarlet...|[bybooks, scarlet...|(1519,[1,2,5,8,11...|  2.0|\n",
      "|bybig citythe act...|Politics|[bybig, citythe, ...|[bybig, citythe, ...|(1519,[5,7,14,15,...|  2.0|\n",
      "|art design notebo...|Politics|[art, design, not...|[art, design, not...|(1519,[1,5,8,10,1...|  2.0|\n",
      "|wilson sectionssk...|Politics|[wilson, sections...|[wilson, sections...|(1519,[1,5,8,11,1...|  2.0|\n",
      "|cover photo credi...|Politics|[cover, photo, cr...|[cover, photo, cr...|(1519,[1,5,8,10,1...|  2.0|\n",
      "|italy artists jas...|Politics|[italy, artists, ...|[italy, artists, ...|(1519,[1,5,8,12,1...|  2.0|\n",
      "|stories politics ...|Politics|[stories, politic...|[stories, politic...|(1519,[1,2,4,7,8,...|  2.0|\n",
      "|bythe stories pol...|Politics|[bythe, stories, ...|[bythe, stories, ...|(1519,[1,2,8,9,15...|  2.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "###String Indexer\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "\n",
    "#Creating\n",
    "pipeline = Pipeline(stages=[regxTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.select('label').distinct().show()\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training set: 81\n",
      "Number of Test set: 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###Splitting the data into train and test set\n",
    "\n",
    "(trainingsplit, testsplit) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Number of Training set: \" + str(trainingsplit.count()))\n",
    "print(\"Number of Test set: \" + str(testsplit.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####Naive Bayes Model\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "###Creating the model\n",
    "naiveBayesModel= NaiveBayes(smoothing=1)\n",
    "\n",
    "###Fitting the model with train set\n",
    "model = naiveBayesModel.fit(trainingsplit)\n",
    "\n",
    "##Predicting the output for test set\n",
    "predictions_output = model.transform(testsplit)\n",
    "type(predictions_output)\n",
    "\n",
    "predictions.registerTempTable('out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8784940984940983"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "##Evaluating out predicted output for accuracy \n",
    "\n",
    "evaluating = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluating.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|             Article|Category|               words|            filtered|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|book review nonfi...|Business|[book, review, no...|[book, review, no...|(1519,[1,2,3,5,8,...|  0.0|[-1.0407086815508...|[0.06865558858659...|       1.0|\n",
      "|byop ed contribut...|Business|[byop, ed, contri...|[byop, ed, contri...|(1519,[2,3,5,15,1...|  0.0|[-0.2716502288934...|[0.14468217128994...|       2.0|\n",
      "|byop ed obamaby j...|Business|[byop, ed, obamab...|[byop, ed, obamab...|(1519,[1,2,3,8,14...|  0.0|[-0.5333269610217...|[0.09317109903603...|       2.0|\n",
      "|bythe stonehow de...|  Movies|[bythe, stonehow,...|[bythe, stonehow,...|(1519,[0,1,2,4,5,...|  1.0|[0.07117607104919...|[0.23676784273580...|       2.0|\n",
      "|news rio grandeby...|Politics|[news, rio, grand...|[news, rio, grand...|(1519,[0,1,4,5,8,...|  2.0|[-0.6492660750169...|[0.10852836929366...|       1.0|\n",
      "|webber perfection...|Business|[webber, perfecti...|[webber, perfecti...|(1519,[1,2,3,5,8,...|  0.0|[-0.9896111563566...|[0.06876656708472...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Logisitc Regression Model without TFIDF\n",
    "\n",
    "## Craeting Logistic Regression Model\n",
    "logisticRegressionModel = LogisticRegression(maxIter=20, regParam=0.8, elasticNetParam=0)\n",
    "\n",
    "## Fitting the model with training set \n",
    "lrModel = logisticRegressionModel.fit(trainingsplit)\n",
    "\n",
    "## Predicting the output for the test set\n",
    "predictions1_output = lrModel.transform(testsplit)\n",
    "\n",
    "\n",
    "\n",
    "predictions1_output.registerTempTable('out1')\n",
    "\n",
    "##Wrong predictions:\n",
    "\n",
    "sqlContext.sql('select *  from out1 where label<>prediction').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8784940984940983"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "### Evaluting the predictions\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|             Article|Category|               words|            filtered|         rawFeatures|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|australia office ...|Politics|[australia, offic...|[australia, sex, ...|(10000,[1,70,88,2...|(10000,[1,70,88,2...|  2.0|\n",
      "|italy artists jas...|Politics|[italy, artists, ...|[italy, artists, ...|(10000,[69,77,110...|(10000,[69,77,110...|  2.0|\n",
      "|art design notebo...|Politics|[art, design, not...|[art, design, not...|(10000,[69,77,110...|(10000,[69,77,110...|  2.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "##Using TF-IDF for logistic regression\n",
    "\n",
    "\n",
    "hTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline1 = Pipeline(stages=[regxTokenizer, stopwordsRemover, hTF, idf, label_stringIdx])\n",
    "pFit = pipeline1.fit(data)\n",
    "data_set = pFit.transform(data)\n",
    "\n",
    "data_set.show(3)\n",
    "data_set.registerTempTable('dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10000,[1,70,88,2...|\n",
      "|(10000,[69,77,110...|\n",
      "|(10000,[69,77,110...|\n",
      "|(10000,[41,60,219...|\n",
      "|(10000,[21,47,76,...|\n",
      "|(10000,[8,26,87,1...|\n",
      "|(10000,[68,78,87,...|\n",
      "|(10000,[2,103,132...|\n",
      "|(10000,[9,87,93,1...|\n",
      "|(10000,[15,21,90,...|\n",
      "|(10000,[68,78,87,...|\n",
      "|(10000,[9,87,93,1...|\n",
      "|(10000,[8,26,87,1...|\n",
      "|(10000,[41,60,219...|\n",
      "|(10000,[69,77,110...|\n",
      "|(10000,[23,37,59,...|\n",
      "|(10000,[23,37,59,...|\n",
      "|(10000,[69,77,110...|\n",
      "|(10000,[1,8,21,60...|\n",
      "|(10000,[68,76,132...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select features from dataset ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                       Article|Category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|imageone wework spaces manh...|Business|[0.9897503661665389,0.00580...|  0.0|       0.0|\n",
      "|bybusiness pressapril hope ...|Business|[0.9784120703904606,0.00709...|  0.0|       0.0|\n",
      "|bybusiness pressapril growt...|Business|[0.9768403506839897,0.01026...|  0.0|       0.0|\n",
      "|business day business highl...|Business|[0.9695175534827642,0.01236...|  0.0|       0.0|\n",
      "|business day business highl...|Business|[0.967015553130405,0.011364...|  0.0|       0.0|\n",
      "|business day bull mobile ou...|Business|[0.9591095118497532,0.01895...|  0.0|       0.0|\n",
      "|business day cohn departure...|Business|[0.9529455876084199,0.01579...|  0.0|       0.0|\n",
      "|weddings business pleasure ...|Business|[0.9391091853882036,0.02723...|  0.0|       0.0|\n",
      "|bybusiness pressapril fight...|Business|[0.937546619312404,0.028603...|  0.0|       0.0|\n",
      "|business day life weworld d...|Business|[0.9363869550753503,0.02879...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Logistic regression model with TF-IDF\n",
    "\n",
    "## Train test split\n",
    "(trainingsplit, testsplit) = data_set.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "##Creating the model\n",
    "logisticRegressionModel = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "###Fitting the model for train set\n",
    "lrModel = logisticRegressionModel.fit(trainingsplit)\n",
    "\n",
    "### Testing the model for test set\n",
    "predictions = lrModel.transform(testsplit)\n",
    "predictions.registerTempTable('logTable')\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Article\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|             Article|Category|               words|            filtered|         rawFeatures|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|book review nonfi...|Business|[book, review, no...|[book, review, no...|(10000,[76,174,20...|(10000,[76,174,20...|  0.0|[-0.0603265388179...|[0.23203750947269...|       2.0|\n",
      "|byop ed contribut...|Business|[byop, ed, contri...|[byop, ed, contri...|(10000,[47,76,118...|(10000,[47,76,118...|  0.0|[-0.0701328524539...|[0.19025038455620...|       2.0|\n",
      "|byop ed obamaby j...|Business|[byop, ed, obamab...|[byop, ed, obamab...|(10000,[15,45,110...|(10000,[15,45,110...|  0.0|[-0.5549428750840...|[0.09650938966760...|       2.0|\n",
      "|bythe stonehow de...|  Movies|[bythe, stonehow,...|[bythe, stonehow,...|(10000,[103,132,1...|(10000,[103,132,1...|  1.0|[0.42012184801817...|[0.33357214135565...|       0.0|\n",
      "|news rio grandeby...|Politics|[news, rio, grand...|[news, rio, grand...|(10000,[95,115,13...|(10000,[95,115,13...|  2.0|[-0.8306569972948...|[0.08882866812598...|       1.0|\n",
      "|webber perfection...|Business|[webber, perfecti...|[webber, perfecti...|(10000,[76,174,20...|(10000,[76,174,20...|  0.0|[0.02432703148983...|[0.24319503356898...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select * from logTable where label<>prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572486772486771"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluting the performance \n",
    "\n",
    "evaluating = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluating.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
