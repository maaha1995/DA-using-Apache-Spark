{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mahalakshmimaddu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mahalakshmimaddu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mahalakshmimaddu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "PythonRDD[82] at RDD at PythonRDD.scala:48\n",
      "PythonRDD[90] at RDD at PythonRDD.scala:48\n",
      "PythonRDD[98] at RDD at PythonRDD.scala:48\n",
      "PythonRDD[106] at RDD at PythonRDD.scala:48\n",
      "PythonRDD[114] at RDD at PythonRDD.scala:48\n",
      "[u'percent', u'business', u'mobile', u'company', u'companies', u'points', u'share', u'sprint', u'stocks', u'facebook', u'businesses', u'deal', u'oil', u'quarter', u'trade', u'apple', u'job', u'industry', u'move', u'twitter']\n",
      "['percent', 'business', 'mobile', 'company', 'companies', 'points', 'share', 'sprint', 'stocks', 'facebook', 'businesses', 'deal', 'oil', 'quarter', 'trade', 'apple', 'job', 'industry', 'move', 'twitter']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc =SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from operator import add\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"advertisement\")\n",
    "stop_words.add(\"said\")\n",
    "stop_words.add(\"\")\n",
    "stop_words.add(\" \")\n",
    "stop_words.add(\"  \")\n",
    "stop_words.add(\"ms\")\n",
    "stop_words.add(\"house\")\n",
    "stop_words.add(\"brown\")\n",
    "stop_words.add(\"people\")\n",
    "#stop_words.add(\"olympic\")\n",
    "stop_words.add(\"year\")\n",
    "stop_words.add(\"years\")\n",
    "stop_words.add(\"time\")\n",
    "stop_words.add(\"state\")\n",
    "stop_words.add(\"states\")\n",
    "stop_words.add(\"women\")\n",
    "stop_words.add(\"gallon\")\n",
    "stop_words.add(\"juan\")\n",
    "stop_words.add(\"trump\")\n",
    "stop_words.add(\"york\")\n",
    "stop_words.add(\"rico\")\n",
    "stop_words.add(\"puerto\")\n",
    "stop_words.add(\"san\")\n",
    "stop_words.add(\"island\")\n",
    "\n",
    "filepath = \"/Users/mahalakshmimaddu/Desktop/DICLab3/P5/Test_Data/BusinessTest/\"\n",
    "\n",
    "for file in os.listdir(filepath):\n",
    "    textRDD = sc.textFile(filepath)\n",
    "    words = textRDD.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n",
    "    print(str(words))\n",
    "    wordcount = words.reduceByKey(add).collect()\n",
    "\n",
    "output = sorted(wordcount, key = lambda x:x[1], reverse=True)\n",
    "\n",
    "top_words = []\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "i = 0;\n",
    "x = 0;\n",
    "\n",
    "while x < len(output) and i < 20:\n",
    "    word = [output[x][0].encode('ascii','ignore')]\n",
    "    if(bool(word[0].strip())):\n",
    "        l1 =  nltk.tag.pos_tag(word)\n",
    "        if ((is_noun(l1[0][1])) and (not (word[0].lower() in (stop_words))) and len(word[0]) > 2):\n",
    "            top_words.append(output[x][0])\n",
    "            i = i+1;\n",
    "            x = x+1;\n",
    "        else:\n",
    "            x = x+1;\n",
    "    else:\n",
    "        x = x+1;\n",
    "print(str(top_words))\n",
    "\n",
    "new_words = []\n",
    "for i in range(len(top_words)):\n",
    "    new_words.append(top_words[i].encode('utf-8'))\n",
    "print(str(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
